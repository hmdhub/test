一.
1.搜素引擎
2.robots.txt
3.Sitemap.xml
4.用户代理
5.降低
二
1.√
2.√
3.√
4.×
5.×
三.
过程
聚焦爬虫：聚焦爬虫需要根据一定的网页分析算法过滤与主题无关的链接，保留有用的链接，
并将其放入等待抓取的url队列。然后，他将根据一定的搜素策略从队列中选择下一步要抓取的网页url,
并重复上述过程，直到达到系统的某一条件时就停止。
通用爬虫：从一个或者多个初始网页的url开始，获得初始网页上的url,在爬取网页的过程中，
不断地从当前页面抽取新的url放入队列，直到满足系统的一定停止条件。
四.
策略：
1.伪装浏览器发送请求 User-agent
2.使用代理IP
3.降低访问频率
4.验证码限制
